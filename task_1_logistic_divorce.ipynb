{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMHjtVPbyaKP"
   },
   "source": [
    "## Logistic Regression Model from scratch for Divorce Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJi26z8awmSD"
   },
   "source": [
    "### Logistic regression\n",
    "Logistic regression uses an equation as the representation, very much like linear regression.\n",
    "\n",
    "Input values (x) are combined linearly using weights or coefficient values (referred to as W) to predict an output value (y). A key difference from linear regression is that the output value being modeled is a binary values (0 or 1) rather than a numeric value.<br>\n",
    "\n",
    "#####  $\\hat{y}$ (w, x) = 1/(1+exp^-(w_0 + w_1 * x_1 + ... + w_p * x_ps))\n",
    "\n",
    "#### Dataset\n",
    "The dataset is available at <strong>\"data/divorce.csv\"</strong> in the respective challenge's repo.<br>\n",
    "<strong>Original Source:</strong> https://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set. Dataset is based on rating for questionnaire filled by people who already got divorse and those who is happily married.<br><br>\n",
    "\n",
    "[//]: # \"The dataset is available at http://archive.ics.uci.edu/ml/machine-learning-databases/00520/data.zip. Unzip the file and use either CSV or xlsx file.<br>\"\n",
    "\n",
    "\n",
    "#### Features (X)\n",
    "1. Atr1 - If one of us apologizes when our discussion deteriorates, the discussion ends. (Numeric | Range: 0-4)\n",
    "2. Atr2 - I know we can ignore our differences, even if things get hard sometimes. (Numeric | Range: 0-4)\n",
    "3. Atr3 - When we need it, we can take our discussions with my spouse from the beginning and correct it. (Numeric | Range: 0-4)\n",
    "4. Atr4 - When I discuss with my spouse, to contact him will eventually work. (Numeric | Range: 0-4)\n",
    "5. Atr5 - The time I spent with my wife is special for us. (Numeric | Range: 0-4)\n",
    "6. Atr6 - We don't have time at home as partners. (Numeric | Range: 0-4)\n",
    "7. Atr7 - We are like two strangers who share the same environment at home rather than family. (Numeric | Range: 0-4)\n",
    "\n",
    "&emsp;.<br>\n",
    "&emsp;.<br>\n",
    "&emsp;.<br>\n",
    "\n",
    "54. Atr54 - I'm not afraid to tell my spouse about her/his incompetence. (Numeric | Range: 0-4)\n",
    "<br><br>\n",
    "Take a look above at the source of the original dataset for more details.\n",
    "\n",
    "#### Target (y)\n",
    "55. Class: (Binary | 1 => Divorced, 0 => Not divorced yet)\n",
    "\n",
    "#### Objective\n",
    "To gain understanding of logistic regression through implementing the model from scratch\n",
    "\n",
    "#### Tasks\n",
    "- Download and load the data (csv file contains ';' as delimiter)\n",
    "- Define X matrix (independent features) and y vector (target feature) as numpy arrays\n",
    "- Add column at position 0 with all values=1 (pandas.DataFrame.insert function). This is for input to the bias $w_0$\n",
    "- Print the shape and datatype of both X and y\n",
    "[//]: # \"- Dataset contains missing values, hence fill the missing values (NA) by performing missing value prediction\"\n",
    "[//]: # \"- Since the all the features are in higher range, columns can be normalized into smaller scale (like 0 to 1) using different methods such as scaling, standardizing or any other suitable preprocessing technique (sklearn.preprocessing.StandardScaler)\"\n",
    "- Split the dataset into 85% for training and rest 15% for testing (sklearn.model_selection.train_test_split function)\n",
    "- Follow code cells to implement simple logistic regression from scratch\n",
    "    - Write hypothesis function to predict values\n",
    "    - Write function for calculating cross entropy loss (or log loss)\n",
    "    - Write function to return gradients for given weights\n",
    "    - Perform gradient descent taking help of above functions\n",
    "    - Write function for calculating accuracy\n",
    "- Train the model using training set using the function implementation\n",
    "- Predict the output for testing set samples and compute accuracy\n",
    "\n",
    "#### Further Fun (will not be evaluated)\n",
    "- Play with learning rate and max_iterations\n",
    "- Testing between whether label encoder vs one hot encoder for categorical features gives better results.\n",
    "- Running model with different feature scaling methods (i.e. scaling, normalization, standardization etc using sklearn)\n",
    "- Training model with different sizes of dataset splitting such as 60-40, 50-50, 70-30, 80-20, 90-10, 95-5 etc.\n",
    "- Shuffling of training samples with different random seed values in the train_test_split function. Check the model error for the testing data for each setup.\n",
    "- Write functions for other classification metrics such as confusion matrix,  precision, recall and f1 scores.\n",
    "\n",
    "\n",
    "#### Helpful links\n",
    "- How Logistic Regression works: https://machinelearningmastery.com/logistic-regression-for-machine-learning/\n",
    "- Feature Scaling: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "- Training testing splitting: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "- Use slack for doubts: https://join.slack.com/t/deepconnectai/shared_invite/zt-givlfnf6-~cn3SQ43k0BGDrG9_YOn4g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21J6cpd_wmSE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4SL1fdNt1k3Q"
   },
   "outputs": [],
   "source": [
    "# Download the dataset from the source\n",
    "!wget _URL_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 963,
     "status": "ok",
     "timestamp": 1597993617875,
     "user": {
      "displayName": "Sridhar Swaminathan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgvUjpWNY_tn9Efzip10QlZv44o7sSIu_Q4keU8=s64",
      "userId": "13395525706176391075"
     },
     "user_tz": -330
    },
    "id": "8VrbXJzI1l4Q"
   },
   "outputs": [],
   "source": [
    "# Unzip the file to the local cloud directory\n",
    "!unzip _filename_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9av7W-wowmSI"
   },
   "outputs": [],
   "source": [
    "# Read the data from local cloud directory\n",
    "\n",
    "# Set delimitebr to semicolon(;) in case of unexpected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eV1jGAQxwmSP"
   },
   "outputs": [],
   "source": [
    "# Print the dataframe rows just to see some samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "joRU6dWxwmSR"
   },
   "outputs": [],
   "source": [
    "# Define X (input features) and y (output feature) \n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAyM-CYCwmSU"
   },
   "outputs": [],
   "source": [
    "X_shape = \n",
    "X_type  = \n",
    "y_shape = \n",
    "y_type  = \n",
    "print(f'X: Type-{X_type}, Shape-{X_shape}')\n",
    "print(f'y: Type-{y_type}, Shape-{y_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Expected output: </strong><br><br>\n",
    "\n",
    "X: Type-<class 'numpy.ndarray'>, Shape-(170, 55)<br>\n",
    "y: Type-<class 'numpy.ndarray'>, Shape-(170,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdLIVOm127-z"
   },
   "outputs": [],
   "source": [
    "# Fill the missing values (IF ANY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "En9Kb9dh2-wm"
   },
   "outputs": [],
   "source": [
    "# Perform feature scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8WF-EqO3BEa"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing here\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "acCATJhI3FdH"
   },
   "outputs": [],
   "source": [
    "# Print the shape of features and target of training and testing: X_train, X_test, y_train, y_test\n",
    "X_train_shape = \n",
    "y_train_shape = \n",
    "X_test_shape  = \n",
    "y_test_shape  = \n",
    "\n",
    "print(f\"X_train: {X_train_shape} , y_train: {y_train_shape}\")\n",
    "print(f\"X_test: {X_test_shape} , y_test: {y_test_shape}\")\n",
    "assert (X_train.shape[0]==y_train.shape[0] and X_test.shape[0]==y_test.shape[0]), \"Check your splitting carefully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eSa7cW-NwmSd"
   },
   "source": [
    "##### Let us start implementing logistic regression from scratch. Just follow code cells, see hints if required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will build a LogisticRegression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, lr=0.01, num_iter=100, fit_intercept=True):\n",
    "        #Initialising all the parameters\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.losses= []\n",
    "        \n",
    "    \n",
    "    #Make an add_intercept function which adds a column of ones to X\n",
    "    def add_intercept(self, X):\n",
    "        \n",
    "        #put appropriate shape inside np.ones(())\n",
    "        intercept = np.ones(())\n",
    "        \n",
    "        #now concatenate it with the original X along suitable axis\n",
    "        return np.concatenate((intercept, X), axis=)\n",
    "    \n",
    "    #Define the sigmoid function\n",
    "    def sigmoid(self, z):\n",
    "        \n",
    "        ## Code Starts here\n",
    "       \n",
    "        ## Code ends here\n",
    "    \n",
    "    \n",
    "    # Define the loss function\n",
    "    def loss(self, y_pred, y):\n",
    "        # Use binary cross entropy\n",
    "        \n",
    "        return # Code here #\n",
    "    \n",
    "    \n",
    "    # Defining the fit function which does gradient ascend\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # If fit_intercept is true, we add a column of ones to X\n",
    "        if self.fit_intercept:\n",
    "            X = self.add_intercept(X)\n",
    "        \n",
    "        ## Code starts here\n",
    "        \n",
    "        # weights initialization - pass the appropriate shape\n",
    "        self.theta = np.zeros()\n",
    "        \n",
    "        \n",
    "        # Gradient ascend\n",
    "        for i in range(self.num_iter):\n",
    "            \n",
    "            # z is the hypothesis\n",
    "            # define the hypothesis\n",
    "            z = \n",
    "            \n",
    "            # final predicted output will be found by taking sigmoid of z\n",
    "            y_pred = self.sigmoid(z)\n",
    "            \n",
    "            # define the gradient\n",
    "            gradient = \n",
    "            \n",
    "            #Update the weights\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "           \n",
    "            #Calculating loss and appending it to the losses array\n",
    "            loss = self.loss(y_pred, y)\n",
    "            self.losses.append(loss)\n",
    "    \n",
    "        ## Code ends here\n",
    "    \n",
    "    # Defining the predict_prob function which will predict probabilties of each class by taking dot product of weights and features\n",
    "    def predict_prob(self, X):\n",
    "        \n",
    "        #Adding intercept if fit_intercept is True\n",
    "        if self.fit_intercept:\n",
    "            X = self.add_intercept(X)\n",
    "            \n",
    "        ## Code starts here\n",
    "    \n",
    "        return self.sigmoid()\n",
    "    \n",
    "        ## Code ends here\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # Rounding off the probability to predict the correct class\n",
    "        return self.predict_prob(X).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C2Mn7wfKwmSo"
   },
   "source": [
    "##### Congratulations! You have implemented logistic regression from scratch. Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tvMc0OqwmSp"
   },
   "outputs": [],
   "source": [
    "# Initialise the model\n",
    "model="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZQ8ITUt4b0N"
   },
   "outputs": [],
   "source": [
    "#Fit the model to the training data\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot([i for i in range(len(model.losses))], model.losses)\n",
    "plt.title(\"Loss curve\")\n",
    "plt.xlabel(\"Iteration num\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on test data\n",
    "y_pred = model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function for calculating accuracy\n",
    "def accuracy(y_true,y_pred):\n",
    "    ## Code Starts here\n",
    "    \n",
    "    ## Code ends here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task_1_logistic_divorse.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
